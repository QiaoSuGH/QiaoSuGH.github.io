<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>SDU2020图形学实验记录</title>
    <link href="/2020/09/15/SDU2020%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/09/15/SDU2020%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
    <tags>
      
      <tag>Computer Graphics</tag>
      
      <tag>SDU2020图形学实验记录</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SDU2020ML实验记录</title>
    <link href="/2020/09/12/SDU2020ML%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/09/12/SDU2020ML%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<blockquote><p>实验环境：Octave</p><p>指导书以及数据集：<a href="https://funglee.github.io/ml/ml.html" target="_blank" rel="noopener">https://funglee.github.io/ml/ml.html</a></p></blockquote><hr><h3 id="Lab1-Regression"><a href="#Lab1-Regression" class="headerlink" title="Lab1 Regression"></a>Lab1 Regression</h3><h4 id="2D-Linear-Regression"><a href="#2D-Linear-Regression" class="headerlink" title="2D Linear Regression"></a>2D Linear Regression</h4><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><div class="hljs"><pre><code class="hljs matlab"><span class="hljs-comment">%一元线性回归 使用BGD</span>x = load(<span class="hljs-string">'ex1_1x.dat'</span>);y = load(<span class="hljs-string">'ex1_1y.dat'</span>);<span class="hljs-built_in">figure</span>;<span class="hljs-built_in">plot</span>(x,y,<span class="hljs-string">'o'</span>);ylabel(<span class="hljs-string">'Height in meters'</span>);xlabel(<span class="hljs-string">'Age in years'</span>);m = <span class="hljs-built_in">length</span>(y);x = [<span class="hljs-built_in">ones</span>(m,<span class="hljs-number">1</span>),x];theta = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>);<span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">theta</span> = <span class="hljs-title">bgd</span><span class="hljs-params">(x,y,alpha,iter,theta)</span></span>  <span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span>=<span class="hljs-number">1</span>:iter    m = <span class="hljs-built_in">length</span>(y);    sum = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>); <span class="hljs-comment">%记录偏导</span>    H = x * theta;<span class="hljs-comment">%变成一列</span>    <span class="hljs-comment">%更新theta0 求和i=1:m (H-y) * 1</span><span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span>=<span class="hljs-number">1</span>:m  sum(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>) = sum(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>) + H(<span class="hljs-built_in">i</span>)-y(<span class="hljs-built_in">i</span>);<span class="hljs-keyword">end</span><span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span>=<span class="hljs-number">1</span>:m  sum(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>) = sum(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>) + (H(<span class="hljs-built_in">i</span>)-y(<span class="hljs-built_in">i</span>))*x(<span class="hljs-built_in">i</span>,<span class="hljs-number">2</span>);<span class="hljs-keyword">end</span>theta = theta - (alpha * sum) / m;  <span class="hljs-keyword">end</span><span class="hljs-keyword">end</span>theta = bgd(x,y,<span class="hljs-number">0.07</span>,<span class="hljs-number">1500</span>,theta);<span class="hljs-built_in">hold</span> on; <span class="hljs-built_in">plot</span>(x(:,<span class="hljs-number">2</span>), x*theta, <span class="hljs-string">'-'</span>)<span class="hljs-built_in">legend</span>(<span class="hljs-string">'Training data'</span>, <span class="hljs-string">'Linear regression'</span>)<span class="hljs-built_in">hold</span> off;</code></pre></div><h5 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h5><ul><li>注意到我们在<code>theta = theta - (alpha * sum) / m</code>中除以了<code>m</code>，经过测试如果不除以<code>m</code>，那么结果的图像是没有线的。（？？？）但是这不是cost function的两种形式吗，为什么不除以<code>m</code>就不行？进一步延申：为什么代价函数的形式是这样的？（前面是否可以是任意实数–网上答案是“是”，如何证明？）</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>SDU2020 ML记录</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Regression</title>
    <link href="/2020/09/07/Regression/"/>
    <url>/2020/09/07/Regression/</url>
    
    <content type="html"><![CDATA[<h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><h4 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h4><p>预测宝可梦的CP值是一个典型的回归问题。考虑三个基本步骤：选定model；定义loss function；training data：根据loss function/找到一个最好的model。</p><blockquote><p>我们在选定model时会定义各个参数，而loss function就是这些参数的函数–用以评估这些参数的好坏</p></blockquote><p>在这个例子中我们使用<strong>？？？Model</strong>，接着我们得到了loss function。那么如何求解loss function的最小值从而得到最好的model？–<strong>梯度下降法 Gradient Descent</strong></p><h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><h5 id="基本操作步骤："><a href="#基本操作步骤：" class="headerlink" title="基本操作步骤："></a>基本操作步骤：</h5><p>这节课里只是简单地介绍了一下Gradient Descent是如何操作的：首先从一元函数求极值开始，我们从某点<code>W0</code>开始，根据导数（这里注意使用梯度下降的前提是loss function是可微的）判断<code>W0</code>应该增大或者减小，然后循环进行这样的变化，直到导数为0。这里便有了<strong>learning rate</strong>这个参数–<code>W0</code>的变化每次都是增加或者减少<code>learning rate * 导数值</code>。推广到二元函数的情形也是一致的：分别求两个变量的偏导数。</p><h5 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h5><p>但是以上只是简单的操作/演示Gradient Descent如何进行，还有一些问题：</p><ul><li>显然对于任意一个二元函数来说，可能local optimal 并非global optimal–由于是从某<code>W0</code>点开始的，因此可能在此附近陷入local minima，而得到不全局的minima。为了让Gradient Descent有效我们必须保证loss function没有局部最优解。</li><li>这里延申出一个问题：我们上面定义的loss function是<strong>凸函数</strong>，只有global optimal。所以：<strong>凸函数的性质？</strong></li></ul><h4 id="对于结果的该进"><a href="#对于结果的该进" class="headerlink" title="对于结果的该进"></a>对于结果的该进</h4><h5 id="其他model"><a href="#其他model" class="headerlink" title="其他model"></a>其他model</h5><p>事实上我们可以尝试增加<code>Xi</code>的次数–使用更高阶数去进行拟合。这样结果就是在训练集上我们的error越来越小，（低次的式子是高次的式子的特殊情况。<strong>考虑可视化的表现：使用更高次数的式子去拟合，其对应的function set包含了低次式对应的function set，所以training data上表现更好是自然的</strong>），但是在testing data上的表现却未必。</p><h5 id="overfitting-过拟合-以及-regularization-正则化"><a href="#overfitting-过拟合-以及-regularization-正则化" class="headerlink" title="overfitting 过拟合 以及 regularization 正则化"></a>overfitting 过拟合 以及 regularization 正则化</h5><p>上面的现象便是过拟合。</p><p>我们可以通过regularization来解决这个问题：在原本loss function的基础上增加一项。<strong>这样的结果是函数变得更加平滑，从而结果更好（平滑意味着输出对输入敏感性降低，从而输入的一些noise对于结果的影响更小）</strong></p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/regularization.png" srcset="/img/loading.gif" alt="regularization" style="zoom: 33%;" /><p><strong>（这里的参数需要手动调整，从而使其在平滑与不平滑之间达到一个较好的状态）</strong></p><h4 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h4><ul><li>凸函数 可微 偏导数</li><li>正则化处理过程中增加一项的数学上的理解</li><li>model有哪些？</li></ul><hr><h3 id="对于Gradient-Descent的理解"><a href="#对于Gradient-Descent的理解" class="headerlink" title="对于Gradient Descent的理解"></a>对于Gradient Descent的理解</h3><h4 id="方向导数"><a href="#方向导数" class="headerlink" title="方向导数"></a>方向导数</h4><hr>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
      <category>李宏毅机器学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅2020机器学习课程记录</title>
    <link href="/2020/09/07/%E6%9D%8E%E5%AE%8F%E6%AF%852020%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/09/07/%E6%9D%8E%E5%AE%8F%E6%AF%852020%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
      <category>李宏毅机器学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GAMES101记录</title>
    <link href="/2020/07/17/GAMES101%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/07/17/GAMES101%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Computer Graphics</category>
      
      <category>GAMES101</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图形学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客搭建过程记录</title>
    <link href="/2020/04/24/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/04/24/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>程序设计思维与实践</category>
      
      <category>代码模板</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
